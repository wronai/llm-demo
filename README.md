# ğŸš€ Minimalne LLM + WÅ‚asny Model - Kompletny Guide

## ğŸ¯ **CZÄ˜ÅšÄ† 1: Uruchomienie w 2 minuty**

### Szybki start (minimalne rozwiÄ…zanie)
```bash
# 1. Sklonuj pliki
git clone <your-repo>
cd minimal-llm

# 2. Uruchom wszystko jednÄ… komendÄ…
chmod +x quick-start.sh
./quick-start.sh

# 3. OtwÃ³rz przeglÄ…darkÄ™
# http://localhost:8501 - Streamlit UI
# http://localhost:11434 - Ollama API
```

### Co siÄ™ dzieje pod spodem?
- **Ollama** - pobiera i uruchamia Mistral 7B
- **Streamlit** - prosty chat interface
- **Docker** - wszystko w kontenerach
- **Minimalne zaleÅ¼noÅ›ci** - tylko 3 pakiety Python!

## ğŸ“ **Struktura projektu (minimalna)**
```
minimal-llm/
â”œâ”€â”€ docker-compose.yml       # 1 plik - caÅ‚a infrastruktura
â”œâ”€â”€ Dockerfile              # Minimalne image
â”œâ”€â”€ requirements.txt         # 3 pakiety
â”œâ”€â”€ quick-start.sh          # 1 komenda = peÅ‚ny setup
â””â”€â”€ app/
    â””â”€â”€ main.py             # 50 linijek = peÅ‚ny chat
```

## ğŸ¯ **CZÄ˜ÅšÄ† 2: StwÃ³rz wÅ‚asny model LLM**

### Krok 1: Przygotowanie Å›rodowiska
```bash
# Instalacja zaleÅ¼noÅ›ci do fine-tuningu
pip install -r model_requirements.txt

# Login do Hugging Face (do publikacji)
huggingface-cli login
```

### Krok 2: Przygotowanie danych
```bash
python create_custom_model.py
# Wybierz opcjÄ™ 1: StwÃ³rz sample dataset
```

PrzykÅ‚ad danych treningowych:
```json
[
  {
    "instruction": "Jak nazywa siÄ™ stolica Polski?",
    "input": "",
    "output": "Stolica Polski to Warszawa."
  },
  {
    "instruction": "WyjaÅ›nij czym jest sztuczna inteligencja",
    "input": "",
    "output": "Sztuczna inteligencja (AI) to dziedzina informatyki..."
  }
]
```

### Krok 3: Fine-tuning modelu
```bash
# Uruchom fine-tuning (wymaga GPU)
python create_custom_model.py
# Wybierz opcjÄ™ 2: Fine-tune model

# Lub peÅ‚ny pipeline
python create_custom_model.py
# Wybierz opcjÄ™ 6: PeÅ‚ny pipeline
```

**Optymalizacje dla RTX 3050:**
- 4-bit quantization
- LoRA (Low-Rank Adaptation)
- Batch size = 1
- Gradient accumulation = 4
- Mixed precision (FP16)

### Krok 4: Konwersja do GGUF
```bash
# Automatycznie generowany skrypt
./convert_to_gguf.sh
```

### Krok 5: Stworzenie modelu w Ollama
```bash
# UtwÃ³rz Modelfile
python create_custom_model.py  # wybierz opcjÄ™ 4

# StwÃ³rz model w Ollama
ollama create wronai -f Modelfile

# Uruchom model
ollama run wronai
```

### Uruchamianie skryptu
Skrypt `create_custom_model.py` oferuje interaktywne menu z nastÄ™pujÄ…cymi opcjami:

```bash
python create_custom_model.py
```

DostÄ™pne opcje:
1. StwÃ³rz przykÅ‚adowy dataset
2. Wykonaj fine-tuning modelu
3. Konwertuj model do formatu GGUF
4. UtwÃ³rz Modelfile dla Ollama
5. Opublikuj model na Hugging Face
6. Wykonaj peÅ‚ny pipeline (1-5)

### Wymagania wstÄ™pne
- Python 3.8+
- PyTorch z obsÅ‚ugÄ… CUDA (zalecane)
- Biblioteki wymienione w `model_requirements.txt`
- Konto na [Hugging Face](https://huggingface.co/) (do publikacji modelu)

### RozwiÄ…zywanie problemÃ³w

#### BÅ‚Ä…d skÅ‚adni w skrypcie
JeÅ›li napotkasz bÅ‚Ä…d skÅ‚adni, upewnij siÄ™, Å¼e:
1. UÅ¼ywasz Pythona 3.8 lub nowszego
2. Wszystkie zaleÅ¼noÅ›ci sÄ… zainstalowane
3. Plik nie zostaÅ‚ uszkodzony podczas pobierania

#### Problemy z zaleÅ¼noÅ›ciami
```bash
# UtwÃ³rz i aktywuj Å›rodowisko wirtualne
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate    # Windows

# Zainstaluj zaleÅ¼noÅ›ci
pip install -r model_requirements.txt
```

#### BrakujÄ…ce uprawnienia
JeÅ›li napotkasz problemy z uprawnieniami, sprÃ³buj:
```bash
# Nadaj uprawnienia do wykonywania skryptÃ³w
chmod +x *.sh

# Uruchom z uprawnieniami administratora (jeÅ›li potrzebne)
sudo python create_custom_model.py
```

### Kontrybucja
Zapraszamy do zgÅ‚aszania problemÃ³w i propozycji zmian poprzez Issues i Pull Requests.

# Test modelu
ollama run wronai "CzeÅ›Ä‡! Kim jesteÅ›?"
```

### Krok 6: Publikacja modelu

#### **Opcja A: Ollama Registry**
```bash
# Push do Ollama Library
ollama push wronai

# Teraz kaÅ¼dy moÅ¼e uÅ¼yÄ‡:
ollama pull your-username/wronai
```

#### **Opcja B: Hugging Face Hub**
```bash
# Publikacja na HF
python publish_to_hf.py

# Model dostÄ™pny na:
# https://huggingface.co/your-username/my-custom-mistral-7b
```

#### **Opcja C: Docker Registry**
```bash
# Spakuj do Docker image
docker build -t my-custom-llm .
docker tag my-custom-llm your-registry/my-custom-llm
docker push your-registry/my-custom-llm
```

## ğŸ¯ **CZÄ˜ÅšÄ† 3: Gotowe alternatywy (zero kodu)**

### **1. Najprostsze - Ollama**
```bash
# Instalacja
curl -fsSL https://ollama.ai/install.sh | sh

# Uruchomienie modelu
ollama run mistral:7b-instruct

# API automatycznie na localhost:11434
```

### **2. Hugging Face Inference API**
```python
import requests

headers = {"Authorization": "Bearer YOUR_HF_TOKEN"}
response = requests.post(
    "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1",
    headers=headers,
    json={"inputs": "Hello!"}
)
```

### **3. Groq (ultra szybkie)**
```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_GROQ_KEY",
    base_url="https://api.groq.com/openai/v1"
)

response = client.chat.completions.create(
    model="mistral-7b-instruct",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### **4. Together.ai**
```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_TOGETHER_KEY",
    base_url="https://api.together.xyz/v1"
)

# Kompatybilne z OpenAI API
```

### **5. Modal.com (serverless GPU)**
```python
import modal

stub = modal.Stub("llm-api")

@stub.function(gpu="T4")
def generate(prompt: str):
    # TwÃ³j kod modelu
    return model.generate(prompt)

# Deploy jednÄ… komendÄ…
# modal deploy
```

## ğŸ¯ **CZÄ˜ÅšÄ† 4: Frontend opcje**

### **1. Streamlit (Python)**
```python
import streamlit as st

st.title("My LLM Chat")
prompt = st.text_input("Message:")
if st.button("Send"):
    response = generate(prompt)
    st.write(response)
```

### **2. Gradio (Python)**
```python
import gradio as gr

def chat(message, history):
    response = generate(message)
    history.append([message, response])
    return "", history

gr.ChatInterface(chat).launch()
```

### **3. Next.js + Vercel AI SDK**
```tsx
import { useChat } from 'ai/react'

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat()
  
  return (
    <div>
      {messages.map(m => <div key={m.id}>{m.content}</div>)}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
      </form>
    </div>
  )
}
```

## ğŸ¯ **CZÄ˜ÅšÄ† 5: PorÃ³wnanie rozwiÄ…zaÅ„**

| RozwiÄ…zanie | Setup Time | KÃ³d | Hosting | GPU |
|-------------|------------|-----|---------|-----|
| **Ollama + Streamlit** | 2 min | 50 linijek | Local/Docker | Optional |
| **Hugging Face API** | 30 sec | 5 linijek | Cloud | No |
| **Groq API** | 1 min | 5 linijek | Cloud | No |
| **Modal.com** | 5 min | 20 linijek | Serverless | Auto |
| **Custom Fine-tuning** | 2 hours | 200 linijek | Self-hosted | Required |

## ğŸ› ï¸ **Debugging & Tips**

### Typowe problemy
```bash
# Model nie Å‚aduje siÄ™
docker logs ollama-engine

# Brak GPU
docker run --rm --gpus all nvidia/cuda:11.8-base nvidia-smi

# Port zajÄ™ty
sudo netstat -tlnp | grep 11434

# Restart wszystkiego
docker compose down && docker compose up -d
```

### Optymalizacje RTX 3050
```python
# W fine-tuningu
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,        # 4-bit quantization
    torch_dtype=torch.float16 # Half precision
)

# Training args
TrainingArguments(
    per_device_train_batch_size=1,   # MaÅ‚y batch
    gradient_accumulation_steps=4,   # Gradients accumulation
    fp16=True                        # Mixed precision
)
```

### Monitoring zasobÃ³w
```bash
# GPU monitoring
watch -n 1 nvidia-smi

# Container resources
docker stats

# Model memory usage
docker exec -it ollama-engine ollama ps
```

## ğŸ¯ **NastÄ™pne kroki**

### Dla nauki:
1. **Eksperymentuj z rÃ³Å¼nymi modelami** - Llama, CodeLlama, Phi-3
2. **Testuj rÃ³Å¼ne techniki fine-tuningu** - LoRA, QLoRA, Full fine-tuning
3. **Dodaj RAG** - Retrieval Augmented Generation
4. **StwÃ³rz multi-agent system**

### Dla produkcji:
1. **PrzejdÅº na managed service** - Groq, Together.ai
2. **Setup monitoring** - LangSmith, Weights & Biases
3. **Dodaj cache** - Redis dla odpowiedzi
4. **Implement rate limiting**

### Dla biznesu:
1. **Fine-tune na wÅ‚asnych danych**
2. **Setup A/B testing** rÃ³Å¼nych modeli
3. **Dodaj feedback loop** od uÅ¼ytkownikÃ³w
4. **Monetize API**

## ğŸ‰ **Podsumowanie**

**Wybierz opcjÄ™ wedÅ‚ug potrzeb:**

- **Demo/nauka**: Ollama + Streamlit (to rozwiÄ…zanie)
- **Prototyp**: Hugging Face API + Gradio
- **MVP**: Groq API + Next.js
- **Produkcja**: Modal/RunPod + custom frontend
- **Enterprise**: Fine-tuned model + wÅ‚asna infrastruktura

**Minimalne rozwiÄ…zanie = 5 plikÃ³w, 50 linijek kodu, 2 minuty setup!**

